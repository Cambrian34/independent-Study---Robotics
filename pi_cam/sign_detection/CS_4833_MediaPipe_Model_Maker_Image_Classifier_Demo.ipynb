{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Srn1S6xtMEm_"
      },
      "source": [
        "# Image classification model customization guide"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This python notebook will help you retrain a model which will be used later in our CS-4833 class.  \n",
        "There are some optional documents you can read:\n",
        "\n",
        "* [MediaPipe website](https://developers.google.com/mediapipe)\n",
        "* [Image classification guide for Python](https://developers.google.com/mediapipe/solutions/vision/image_classifier/python)\n",
        "* [Image classification for raspberry pi GitHub](https://github.com/googlesamples/mediapipe/tree/main/examples/image_classification/raspberry_pi)"
      ],
      "metadata": {
        "id": "B4XY-YdIiSMH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JO1GUwC1_T2x"
      },
      "outputs": [],
      "source": [
        "#@title License information\n",
        "# Copyright 2023 The MediaPipe Authors.\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "#\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KL9uA_GxMEdo"
      },
      "source": [
        "The MediaPipe Model Maker package is a low-code solution for customizing on-device machine learning (ML) Models.\n",
        "\n",
        "The MediaPipe image classification solution provides several models you can use immediately for machine learning (ML) in your application. However, if you need to classify images with content not covered by the provided models, you can customize any of the provided models with your own data and MediaPipe [Model Maker](https://developers.google.com/mediapipe/solutions/model_maker). This model modification tool rebuilds a portion of the model using data you provide. This method is faster than training a new model and can produce a model that is more useful for your specific application.\n",
        "\n",
        "The following sections show you how to use Model Maker to retrain a pre-built model for image classification with your own data, which you can then use with the MediaPipe [Image Classifier](https://developers.google.com/mediapipe/solutions/vision/image_classifier). The example retrains a general purpose classification model to classify images of flowers.\n",
        "\n",
        "This notebook shows the end-to-end process of customizing an ImageNet pretrained image classification model for recognizing flowers defined in a user customized flower dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZRdscPmRAq8"
      },
      "source": [
        "## Setup\n",
        "\n",
        "This section describes key steps for setting up your development environment to retrain a model. These instructions describe how to update a model using [Google Colab](https://colab.research.google.com/), and you can also use Python in your own development environment. For general information on setting up your development environment for using MediaPipe, including platform version requirements, see the [Setup guide for Python](https://developers.google.com/mediapipe/solutions/setup_python).\n",
        "\n",
        "To install the libraries for customizing a model, run the following commands:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plvO-YmcQn5g"
      },
      "outputs": [],
      "source": [
        "!python --version\n",
        "!pip install --upgrade pip\n",
        "!pip install mediapipe-model-maker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nbu3mnPiSvSn"
      },
      "source": [
        "Use the following code to import the required Python classes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cG2McL-NEOpI"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "import tensorflow as tf\n",
        "assert tf.__version__.startswith('2')\n",
        "\n",
        "from mediapipe_model_maker import image_classifier\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7rcwtC5TQQ1"
      },
      "source": [
        "## Prepare data\n",
        "\n",
        "Retraining a model for image classification requires a dataset that includes all kinds of items, or *classes*, that you want the completed model to be able to identify. You can do this by trimming down a public dataset to only the classes that are relevant to your usecase, compiling your own data, or some combination of both. The dataset can be significantly smaller that what would be required to train a new model."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Upload our dataset\n",
        "\n",
        "* Step 1: Organize our own dataset by folloing this pattern: `<image_path>/<label_name>/<image_names>.*`. The directory contains several subdirectories, each corresponding to specific class labels.\n",
        "* Step 2: Compress your dataset folder to a zip file\n",
        "* Step 3: Upload your zip file to this colab runtime\n",
        "* Step 4: Uncompress it using the following code\n",
        "    `!unzip dataset.zip`"
      ],
      "metadata": {
        "id": "Y1ji9Kgxjt6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# uncompress the zip file, change the name if you use a different name\n",
        "!unzip -o dataset.zip"
      ],
      "metadata": {
        "id": "bac7S2I7dv8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# specify our image_path to the folder name, change the name if you use a different name\n",
        "image_path = \"dataset\""
      ],
      "metadata": {
        "id": "azReUAyYeJa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QghwXAtTCGO"
      },
      "source": [
        "### Review data\n",
        "\n",
        "When preparing data for training with Model Maker, you should review the training data to make sure it is in the proper format, correctly classified, and organized in directories corresponding to classification labels. *This step is optional, but recommended.*\n",
        "\n",
        "The following code block retrieves all the label names from the expected directory structure at `image_path` and prints them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65jYI0XtyiJC"
      },
      "outputs": [],
      "source": [
        "print(image_path)\n",
        "labels = []\n",
        "for i in os.listdir(image_path):\n",
        "    if os.path.isdir(os.path.join(image_path, i)):\n",
        "        labels.append(i)\n",
        "print(labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdVEhza1KkbJ"
      },
      "source": [
        "You can review a few of the example images from each category using the following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cqj6ydhnGsgy"
      },
      "outputs": [],
      "source": [
        "NUM_EXAMPLES = 2\n",
        "\n",
        "for label in labels:\n",
        "  label_dir = os.path.join(image_path, label)\n",
        "  example_filenames = os.listdir(label_dir)[:NUM_EXAMPLES]\n",
        "  fig, axs = plt.subplots(1, NUM_EXAMPLES, figsize=(4,3))\n",
        "  for i in range(NUM_EXAMPLES):\n",
        "    axs[i].imshow(plt.imread(os.path.join(label_dir, example_filenames[i])))\n",
        "    axs[i].get_xaxis().set_visible(False)\n",
        "    axs[i].get_yaxis().set_visible(False)\n",
        "  fig.suptitle(f'Showing {NUM_EXAMPLES} examples for {label}')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMAShPQWUsbc"
      },
      "source": [
        "### Create dataset\n",
        "\n",
        "Training data for machine learning can be large, consisting of hundreds or thousands of files which typically do not fit into available memory. You must also split it into groups for different uses: training, testing, and validation. For these reasons, Model Maker uses a `Dataset` class to organize training data and feed it to the retraining process.\n",
        "\n",
        "To create a dataset, use the `Dataset.from_folder` method to load the data located at `image_path` and split it into training, testing, and validation groups:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uELMchkggUMP"
      },
      "outputs": [],
      "source": [
        "data = image_classifier.Dataset.from_folder(image_path)\n",
        "train_data, remaining_data = data.split(0.8)\n",
        "test_data, validation_data = remaining_data.split(0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6nWxtN-QZGj"
      },
      "source": [
        "In this example, 80% of the data is used for training, with the remaining data split in half, so that 10% of the total is used for testing, and 10% for validation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhUcNVuLUzB1"
      },
      "source": [
        "## Retrain model\n",
        "\n",
        "Once you have completed preparing your data, you can begin retraining a model to build a new classification layer that can recognize the items types, or classes, defined by your training data. This type of model modification is called [transfer learning](https://www.wikipedia.org/wiki/Transfer_learning). The instructions below use the data prepared in the previous section to retrain an image classification model to recognize different types of flowers.\n",
        "\n",
        "**Note:** For this type of model, the retraining process causes the model to forget any classes it was previously able to recognize. Once the retraining is complete, the new model can *only* recognize classes trained from the new dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Dv3H2bKO9L9"
      },
      "source": [
        "### Set retraining options\n",
        "\n",
        "There are a few required settings to run a retraining aside from your training dataset: output directory for the model and the model architecture. Use `HParams` object `export_dir` parameter to specify a model output directory. Use the `SupportedModels` class to specify the model architecture. The image classifier solution supports the following model architectures:\n",
        "\n",
        "- `MobileNet-V2`\n",
        "- `EfficientNet-Lite0`\n",
        "- `EfficientNet-Lite2`\n",
        "- `EfficientNet-Lite4`\n",
        "\n",
        "To set the required parameters, use the following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2B0sz4PTMUVZ"
      },
      "outputs": [],
      "source": [
        "spec = image_classifier.SupportedModels.MOBILENET_V2\n",
        "hparams = image_classifier.HParams(export_dir=\"exported_model\", epochs=10)\n",
        "options = image_classifier.ImageClassifierOptions(supported_model=spec, hparams=hparams)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h47-hERKUOks"
      },
      "source": [
        "This example code uses MobileNetV2 model architecture, which you can learn more about from the  [MobileNetV2](https://arxiv.org/abs/1801.04381) research paper. The retraining process has many additional options, however most of them are set for you automatically. You can learn about these optional parameters in the [Retraining parameters](#retraining_parameters) section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxLsv-abU4cr"
      },
      "source": [
        "### Run retraining\n",
        "\n",
        "With your training dataset and retraining options prepared, you are ready to start the retraining process. This process is resource intensive and can take a few minutes to a few hours depending on your available compute resources. Using a Google Colab environment with standard CPU processing, the example retraining below takes about 20 minutes to train on approximately 4000 images. You can typically decrease your training time by using GPU processors.\n",
        "\n",
        "To begin the retraining process, use the `create()` method with dataset and options you previously defined:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CkbUY97gNP2h"
      },
      "outputs": [],
      "source": [
        "model = image_classifier.ImageClassifier.create(\n",
        "    train_data = train_data,\n",
        "    validation_data = validation_data,\n",
        "    options=options,\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0-ZzkN_VH4K"
      },
      "source": [
        "### Evaluate performance\n",
        "\n",
        "After retraining the model, you should evaluate it on a test dataset, which is typically a portion of your original dataset not used during training. Accuracy levels between 0.8 and 0.9 are generally considered very good, but your use case requirements may differ. You should also consider how fast the model can produce an inference. Higher accuracy frequently comes at the cost of longer inference times.\n",
        "\n",
        "To run an evaluation of the example model, run it against the test portion of the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wjMzqWZQ9oV"
      },
      "outputs": [],
      "source": [
        "loss, acc = model.evaluate(test_data)\n",
        "print(f'Test loss:{loss}, Test accuracy:{acc}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wO-WVLaIdlZJ"
      },
      "source": [
        "**Caution:** While high accuracy of a model is a common goal for machine learning models, you should be cautious of training to a point of [overfitting](https://en.wikipedia.org/wiki/Overfitting), which causes the model to perform extremely well with its training data, but quite poorly on new data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kl92JHASVLSX"
      },
      "source": [
        "## Export model\n",
        "\n",
        "After retraining a model, you must export it to Tensorflow Lite model format to use it with the MediaPipe in your application. The export process generates required model metadata, as well as a classification label file.\n",
        "\n",
        "To export the retrained model for use in your application, use the following command:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7sGv08Knh3_D"
      },
      "outputs": [],
      "source": [
        "model.export_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNfK9UnAgwmp"
      },
      "source": [
        "Use the follow commands with Google Colab to list model and download it to your development environment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hKOJXwH57bL"
      },
      "outputs": [],
      "source": [
        "!ls exported_model\n",
        "files.download('exported_model/model.tflite')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfr-8r5gtqNh"
      },
      "source": [
        "## Performance benchmarks\n",
        "\n",
        "Below is a summary of our benchmarking results for the supported model architectures. These models were trained and evaluated on the same flowers dataset as this notebook. When considering the model benchmarking results, there are a few important caveats to keep in mind:\n",
        "- The test accuracy column reflects models which were trained with the default parameters. To optimize model performance, experiment with different model and retraining parameters in order to obtain the highest test accuracy. Refer to the [Retraining parameters](#retraining_parameters) section for more information on customizing these settings.\n",
        "- The larger model architectures, such as EfficientNet_Lite4, may not acheive the highest test accuracy on simpler datasets like the flowers dataset used this notebook. Research suggests that these larger model architecture can outperform the others on more complex datasets like ImageNet, for more information, see [EfficientNet paper](https://arxiv.org/pdf/1905.11946.pdf). The ImageNet dataset is more complex, with over a million training images and 1000 classes, while the flowers dataset has only 3670 training images and 5 classses.\n",
        "\n",
        "<table>\n",
        "<thead>\n",
        "<tr>\n",
        "<th>Model architecture</th>\n",
        "<th>Test Accuracy</th>\n",
        "<th>Model Size</th>\n",
        "<th>CPU 1 Thread Latency(Pixel 6)</th>\n",
        "<th>GPU Latency(Pixel 6)</th>\n",
        "<th>EdgeTPULatency(Pixel 6)</th>\n",
        "</tr>\n",
        "</thead>\n",
        "<tbody>\n",
        "<tr>\n",
        "<td>MobileNet_V2</td>\n",
        "<td>85.4%</td>\n",
        "<td><strong>8.9MB</strong></td>\n",
        "<td>29.12</td>\n",
        "<td>77.77</td>\n",
        "<td>31.14</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>EfficientNet_Lite0</td>\n",
        "<td>91.3%</td>\n",
        "<td>13.5MB</td>\n",
        "<td><strong>15.6</strong></td>\n",
        "<td><strong>9.25</strong></td>\n",
        "<td><strong>16.72</strong></td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>EfficientNet_Lite2</td>\n",
        "<td><strong>91.5%</strong></td>\n",
        "<td>19.2MB</td>\n",
        "<td>35.2</td>\n",
        "<td>13.94</td>\n",
        "<td>37.52</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>EfficientNet_Lite4</td>\n",
        "<td>90.8%</td>\n",
        "<td>46.8MB</td>\n",
        "<td>103.16</td>\n",
        "<td>23.14</td>\n",
        "<td>114.67</td>\n",
        "</tr>\n",
        "</tbody>\n",
        "</table>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}